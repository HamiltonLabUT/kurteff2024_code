{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - Update locally!\n",
    "git_path = '/path/to/git/kurteff2024_code/'\n",
    "data_path = '/path/to/bids/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "import random\n",
    "import itertools as itools\n",
    "\n",
    "from img_pipe import img_pipe\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams as rc\n",
    "import matplotlib.patheffects as PathEffects\n",
    "rc['pdf.fonttype'] = 42\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(git_path,\"analysis\",\"mtrf\"))\n",
    "import mtrf_utils\n",
    "sys.path.append(os.path.join(git_path,\"preprocessing\",\"imaging\"))\n",
    "import imaging_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs = [s for s in os.listdir(\n",
    "    os.path.join(git_path,\"preprocessing\",\"events\",\"csv\")) if \"TCH\" in s or \"S0\" in s]\n",
    "exclude = [\"TCH8\"]\n",
    "no_imaging = [\"S0010\"]\n",
    "subjs = [s for s in subjs if s not in exclude]\n",
    "\n",
    "blocks = {\n",
    "    s: [\n",
    "        b.split(\"_\")[-1] for b in os.listdir(os.path.join(\n",
    "            git_path,\"analysis\",\"events\",\"csv\",s)) if f\"{s}_B\" in b and os.path.isfile(os.path.join(\n",
    "            git_path,\"analysis\",\"events\",\"csv\",s,b,f\"{b}_spkr_sn_all.txt\"\n",
    "        ))\n",
    "    ] for s in subjs\n",
    "}\n",
    "models = ['model1','model2','model3','model4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_data(subj,blocks,git_path,data_path,channel='spkr',level='sn',condition='all',\n",
    "               tmin=-.5, tmax=2, baseline=None, click=False):\n",
    "    epochs = []\n",
    "    for b in blocks:\n",
    "        blockid = f'{subj}_{b}'\n",
    "        raw_fpath = os.path.join(\n",
    "            data_path,f\"sub-{subj}\",blockid,\"HilbAA_70to150_8band\",\"ecog_hilbAA70to150.fif\")\n",
    "        if click:\n",
    "            eventfile = os.path.join(git_path,\"preprocessing\",\"events\",\"csv\",s,blockid,\n",
    "                                     f\"{blockid}_click_eve.txt\")\n",
    "        else:\n",
    "            eventfile = os.path.join(git_path,\"preprocessing\",\"events\",\"csv\",s,blockid,\n",
    "                                     f\"{blockid}_{channel}_{level}_{condition}.txt\")\n",
    "        raw = mne.io.read_raw_fif(raw_fpath,preload=True,verbose=False)\n",
    "        fs = raw.info['sfreq']\n",
    "        if click:\n",
    "            onset_index, offset_index, id_index = 0,2,4\n",
    "        else:\n",
    "            onset_index, offset_index, id_index = 0,1,2\n",
    "        with open(eventfile,'r') as f:\n",
    "            r = csv.reader(f,delimiter='\\t')\n",
    "            events = np.array([[np.ceil(float(row[onset_index])*fs).astype(int),\n",
    "                                np.ceil(float(row[offset_index])*fs).astype(int),\n",
    "                                int(row[id_index])] for row in r])\n",
    "        epochs.append(mne.Epochs(raw,events,tmin=tmin,tmax=tmax,baseline=baseline,preload=True,verbose=False))\n",
    "    return mne.concatenate_epochs(epochs,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LME #1: onset vs. sustained responses\n",
    "\n",
    "Layout:\n",
    "\n",
    "| subj     | elec     |  roi     |  window         |   si     | onset_tmax |\n",
    "|:--------:|:--------:|:--------:|:---------------:|:--------:|:----------:|\n",
    "|  S0006   | RPPST13  |  stg     |{onset,sustained}|  0.523   |   0.323    |\n",
    "|   ...    |   ...    |   ...    |      ...        |   ...    |    ...     |\n",
    "\n",
    "* LME Equation: `si ~ window + roi + (1|subj)`\n",
    "* SI Equations: $SI_{n_{onset}} = \\frac{1}{t}\\sum\\limits_{0}^{0.75}{H_\\gamma L_{n,t}}-{H_\\gamma S_{n,t}}$, and $SI_{n_{sustained}} = \\frac{1}{t}\\sum\\limits_{1}^{1.75}{H_\\gamma L_{n,t}}-{H_\\gamma S_{n,t}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spkr = pd.DataFrame(columns=['subj','elec','roi','window','si','onset_tmax'])\n",
    "df_mic = pd.DataFrame(columns=['subj','elec','roi','window','si','onset_tmax'])\n",
    "onset_df = pd.read_csv(os.path.join(git_path,\"stats\",\"onset_stats.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp_tmin = -.5; erp_tmax = 2; reject = None; baseline = None\n",
    "epochs = dict(); ch_names = dict()\n",
    "for s in tqdm(subjs):\n",
    "    epochs[s] = dict(); spkr_epochs, mic_epochs = [], []\n",
    "    for b in blocks[s]:\n",
    "        blockid = f'{s}_{b}'\n",
    "        raw_fpath = os.path.join(data_path,f\"sub-{s}\",s,blockid,\"HilbAA_70to150_8band\",\n",
    "                                 \"ecog_hilbAA70to150.fif\")\n",
    "        raw = mne.io.read_raw_fif(raw_fpath,preload=True,verbose=False)\n",
    "        ch_names[s] = raw.info['ch_names']; fs = raw.info['sfreq']\n",
    "        # Spkr events\n",
    "        eventfile = os.path.join(git_path,\"preprocessing\",\"events\",\"csv\",s,blockid,\n",
    "                                 f\"{blockid}_spkr_sn_all.txt\")\n",
    "        with open(eventfile,'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            events = np.array([[int(float(row[0])*fs),int(float(row[1])*fs),int(row[2])] for row in c])\n",
    "        spkr_epochs.append(mne.Epochs(raw,events,tmin=erp_tmin,tmax=erp_tmax,\n",
    "                                      baseline=baseline,reject=reject,verbose=False))\n",
    "        # Mic events\n",
    "        eventfile = os.path.join(git_path,\"preprocessing\",\"events\",\"csv\",s,blockid,\n",
    "                                 f\"{blockid}_mic_sn_all.txt\")\n",
    "        with open(eventfile,'r') as f:\n",
    "            c = csv.reader(f,delimiter='\\t')\n",
    "            events = np.array([[int(float(row[0])*fs),int(float(row[1])*fs),int(row[2])] for row in c])\n",
    "        mic_epochs.append(mne.Epochs(raw,events,tmin=erp_tmin,tmax=erp_tmax,\n",
    "                                      baseline=baseline,reject=reject,verbose=False))\n",
    "    epochs[s]['spkr'] = mne.concatenate_epochs(spkr_epochs)\n",
    "    epochs[s]['mic'] = mne.concatenate_epochs(mic_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_tmin, onset_tmax = 0, 0.75; sus_tmin, sus_tmax = 1., 1.75; x = epochs[s]['mic'].times\n",
    "onset_inds = [np.where(x==onset_tmin)[0][0],np.where(x==onset_tmax)[0][0]]\n",
    "sus_inds = [np.where(x==sus_tmin)[0][0],np.where(x==sus_tmax)[0][0]]\n",
    "si = {'onset':{}, 'sustained':{}}\n",
    "for s in tqdm(subjs):\n",
    "    subj_si_ons = []; subj_si_sus = []\n",
    "    for i,ch in enumerate(epochs[s]['mic'].info['ch_names']):\n",
    "        spkr_resp = epochs[s]['spkr'].get_data(picks=[ch]).squeeze()\n",
    "        ons_resp_spkr = spkr_resp[:,onset_inds[0]:onset_inds[1]].mean(0).mean(0)\n",
    "        sus_resp_spkr = spkr_resp[:,sus_inds[0]:sus_inds[1]].mean(0).mean(0)\n",
    "        mic_resp = epochs[s]['mic'].get_data(picks=[ch]).squeeze()\n",
    "        ons_resp_mic = mic_resp[:,onset_inds[0]:onset_inds[1]].mean(0).mean(0)\n",
    "        sus_resp_mic = mic_resp[:,sus_inds[0]:sus_inds[1]].mean(0).mean(0)\n",
    "        subj_si_ons.append(ons_resp_spkr - ons_resp_mic)\n",
    "        subj_si_sus.append(sus_resp_spkr - sus_resp_mic)\n",
    "    si['onset'][s] = np.array(subj_si_ons)\n",
    "    si['sustained'][s] = np.array(subj_si_sus)\n",
    "si_min = np.hstack((np.hstack((list(si['onset'].values()))),\n",
    "                    np.hstack((list(si['sustained'].values()))))).min()\n",
    "si_max = np.hstack((np.hstack((list(si['onset'].values()))),\n",
    "                    np.hstack((list(si['sustained'].values()))))).max()\n",
    "for s in subjs:\n",
    "    si['onset'][s] = ((si['onset'][s]-si_min)/(si_max-si_min)) * 2 - 1\n",
    "    si['sustained'][s] = ((si['sustained'][s]-si_min)/(si_max-si_min)) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['subj','elec','elec_type','roi','roi_condensed','window','si'])\n",
    "for s in subjs:\n",
    "    # Get anat\n",
    "    if s not in no_imaging:\n",
    "        pt = img_pipe.freeCoG(f\"{s}_complete\", hem='stereo', subj_dir=data_path)\n",
    "        anat = pt.get_elecs(elecfile_prefix=\"TDT_elecs_all_warped\")['anatomy']\n",
    "        fs_ch_names = [a[0][0] for a in anat]; fs_rois = [a[3][0] for a in anat]\n",
    "    # Get the onset info\n",
    "    onset_chs_spkr = np.unique([ch for ch in onset_df.loc[onset_df['subj']==s]['ch_name'] if not np.isnan(\n",
    "        onset_df.loc[(onset_df['subj']==s)&(onset_df['ch_name']==ch)&(onset_df['condition']=='spkr')][\n",
    "            'peak_amplitude'].values[0])])\n",
    "    onset_chs_mic = np.unique([ch for ch in onset_df.loc[onset_df['subj']==s]['ch_name'] if not np.isnan(\n",
    "        onset_df.loc[(onset_df['subj']==s)&(onset_df['ch_name']==ch)&(onset_df['condition']=='mic')][\n",
    "            'peak_amplitude'].values[0])])\n",
    "    dual_onset_chs = np.intersect1d(onset_chs_spkr,onset_chs_mic)\n",
    "    no_onset_chs = [ch for ch in epochs[s]['mic'].info['ch_names'] if ch not in list(\n",
    "        onset_chs_spkr) and ch not in list(onset_chs_mic)]\n",
    "    for fif_idx, elec in enumerate(epochs[s]['mic'].info['ch_names']):\n",
    "        ons_si = si['onset'][s][fif_idx]; sus_si = si['sustained'][s][fif_idx]\n",
    "        # Get anatomy\n",
    "        if s not in no_imaging:\n",
    "            if elec in fs_ch_names:\n",
    "                fs_idx = fs_ch_names.index(elec); fs_roi = fs_rois[fs_idx]\n",
    "                roi_condensed = imaging_utils.condense_roi(fs_roi)\n",
    "            else:\n",
    "                warnings.warn(f\"Could not locate anatomy for {s} {elec}\")\n",
    "                fs_roi = \"roi_unavailable\"; roi_condensed = \"roi_unavailable\"\n",
    "        else:\n",
    "            fs_roi = \"roi_unavailable\"; roi_condensed = \"roi_unavailable\"\n",
    "        if elec in onset_chs_spkr:\n",
    "            if elec in dual_onset_chs:\n",
    "                elec_type = \"dual_onset\"\n",
    "            else:\n",
    "                elec_type = \"spkr_only\"\n",
    "        elif elec in onset_chs_mic:\n",
    "            elec_type = \"mic_only\"\n",
    "        else:\n",
    "            elec_type = \"no_onset\"\n",
    "        onset_row = pd.DataFrame({'subj':[s], 'elec':[elec], 'elec_type':[elec_type], 'roi':[fs_roi],\n",
    "            'roi_condensed':[roi_condensed], 'window':['onset'], 'si':[ons_si]})\n",
    "        sustained_row = pd.DataFrame({'subj':[s], 'elec':[elec], 'elec_type':[elec_type], 'roi':[fs_roi],\n",
    "            'roi_condensed':[roi_condensed], 'window':['sustained'], 'si':[sus_si]})\n",
    "        df = df.append(onset_row,ignore_index=True); df = df.append(sustained_row,ignore_index=True)\n",
    "df.to_csv(os.path.join(git_path,'stats','lme','csv','onset_sustained_si.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LME #2: A1, A2, and insular peak latencies\n",
    "\n",
    "This is to get a more finegrained look at the differences between these regions' response profiles.\n",
    "\n",
    "Layout:\n",
    "\n",
    "| subj     | elec     |     roi          | peak_amp | peak_lat |\n",
    "|:--------:|:--------:|:----------------:|:--------:|:--------:|\n",
    "|  S0006   | RPPST13  |   {A1,A2,insula} |  0.523   |  0.323   |\n",
    "|   ...    |   ...    |      ...         |   ...    |   ...    |\n",
    "\n",
    "* LME Equation: `peak_lat ~ roi + (1|subj)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_rois = ['HG', 'PT']; a2_rois = ['STG', 'STS', 'PP', 'MTG']\n",
    "insular_rois = ['insula_inf','insula_sup','insula_post','insula_ant']\n",
    "peak_tmin, peak_tmax = 0, 0.5\n",
    "df = pd.DataFrame(columns=['subj','elec','roi','peak_amp','peak_lat'])\n",
    "excl_df = pd.read_csv(os.path.join(git_path,\"analysis\",\"all_excluded_electrodes.csv\"))\n",
    "for s in tqdm([ss for ss in subjs if ss not in no_imaging]):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        spkr_epochs = epoch_data(s,blocks[s],git_path,data_path,channel='spkr',tmin=peak_tmin,tmax=peak_tmax)\n",
    "    x = spkr_epochs.times\n",
    "    excl_ch_names = [ch for ch in excl_df.loc[excl_df['subject']==s]['channel']]\n",
    "    pt = img_pipe.freeCoG(f\"{s}_complete\", hem='stereo')\n",
    "    anat = pt.get_elecs(elecfile_prefix=\"TDT_elecs_all_warped\")['anatomy']\n",
    "    fs_ch_names = [a[0][0] for a in anat]\n",
    "    fs_rois = [a[3][0] for a in anat]\n",
    "    for elec in spkr_epochs.info['ch_names']:\n",
    "        if elec not in excl_ch_names and elec in fs_ch_names:\n",
    "            fs_idx = fs_ch_names.index(elec)\n",
    "            if imaging_utils.condense_roi(fs_rois[fs_idx]) in a1_rois:\n",
    "                spkr_resp = spkr_epochs.get_data(picks=[elec]).squeeze().mean(0) # Avg across epochs\n",
    "                spkr_peak_amp = spkr_resp.max()\n",
    "                spkr_latency_idx = spkr_resp.argmax(); spkr_peak_latency = x[spkr_latency_idx]\n",
    "                spkr_row = pd.DataFrame({'subj':[s], 'elec':[elec], 'roi':['a1'], 'cond':['spkr'],\n",
    "                    'peak_amp':[spkr_peak_amp], 'peak_lat':[spkr_peak_latency]})\n",
    "                df = df.append(spkr_row,ignore_index=True)\n",
    "            if imaging_utils.condense_roi(fs_rois[fs_idx]) in a2_rois:\n",
    "                spkr_resp = spkr_epochs.get_data(picks=[elec]).squeeze().mean(0) # Avg across epochs\n",
    "                spkr_peak_amp = spkr_resp.max()\n",
    "                spkr_latency_idx = spkr_resp.argmax(); spkr_peak_latency = x[spkr_latency_idx]\n",
    "                spkr_row = pd.DataFrame({'subj':[s], 'elec':[elec], 'roi':['a2'], 'cond':['spkr'],\n",
    "                    'peak_amp':[spkr_peak_amp], 'peak_lat':[spkr_peak_latency]})\n",
    "                df = df.append(spkr_row,ignore_index=True)\n",
    "            if imaging_utils.condense_roi(fs_rois[fs_idx]) in a2_rois:\n",
    "                spkr_resp = spkr_epochs.get_data(picks=[elec]).squeeze().mean(0)\n",
    "                spkr_peak_amp = spkr_resp.max()\n",
    "                spkr_latency_idx = spkr_resp.argmax(); spkr_peak_latency = x[spkr_latency_idx]\n",
    "                spkr_row = pd.DataFrame({'subj':[s], 'elec':[elec], 'roi':['insular'], 'cond':['spkr'],\n",
    "                    'peak_amp':[spkr_peak_amp], 'peak_lat':[spkr_peak_latency]})\n",
    "                df = df.append(spkr_row,ignore_index=True)\n",
    "df.to_csv(os.path.join(git_path,\"stats\",\"lme\",\"csv\",\"temporal_vs_insular_spkr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LME #3: mTRF model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_folder = os.path.join(git_path,\"analysis\",\"mtrf\",\"h5\")\n",
    "features = {model_number:mtrf_utils.get_feats(\n",
    "    model_number,extend_labels=True,mode='ecog') for model_number in models}\n",
    "n_feats = {model_number:len(features[model_number]) for model_number in models}\n",
    "# Make delays\n",
    "delay_min, delay_max = -0.3, 0.5\n",
    "delays = np.arange(np.floor(delay_min*100),np.ceil(delay_max*100),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load strf data from pandas dataframe\n",
    "rfpath = os.path.join(git_path,\"analysis\",\"mtrf\",\"results.csv\")\n",
    "df = pd.read_csv(rfpath)\n",
    "wts, corrs, pvals, sig_wts, sig_corrs, alphas = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "for m in models:\n",
    "    wts[m], corrs[m], pvals[m], sig_wts[m], sig_corrs[m], alphas[m] = dict(), dict(), dict(), dict(), dict(), dict()\n",
    "    b = tqdm(subjs)\n",
    "    for s in b:\n",
    "        b.set_description(f\"Loading STRF for {s} {m}\")\n",
    "        with h5py.File(f\"{h5_folder}weights/{s}_weights.hdf5\",'r') as f:\n",
    "            wts[m][s] = np.array(f.get(m))\n",
    "        pvals[m][s], corrs[m][s], alphas[m][s] = [], [], []\n",
    "        for i, rs in enumerate(df['subject']):\n",
    "            if rs == s and df['model'][i] == m:\n",
    "                corrs[m][s].append(df['r_value'][i]); pvals[m][s].append(df['p_value'][i])\n",
    "                alphas[m][s].append(df['best_alpha'][i])\n",
    "        corrs[m][s] = np.array(corrs[m][s]); pvals[m][s] = np.array(pvals[m][s])\n",
    "        alphas[m][s] = np.array(alphas[m][s]); n_delays, n_feats, n_chans = wts[m][s].shape\n",
    "        if s == \"TCH14\":\n",
    "            n_chans -= 1 # rm EKG channel\n",
    "        sig_wts[m][s] = np.ones(wts[m][s].shape); sig_corrs[m][s] = np.ones(n_chans)\n",
    "        for i in np.arange(n_chans):\n",
    "            if pvals[m][s][i] < 0.01:\n",
    "                sig_wts[m][s][:,:,i] = wts[m][s][:,:,i]; sig_corrs[m][s][i] = corrs[m][s][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update xm/ym to make a different csv\n",
    "xm, ym = 'model1', 'model2'\n",
    "header = [['r','model','subject','channel']]\n",
    "for s in subjs:\n",
    "    ch_names = mne.io.read_raw_fif(os.path.join(data_path,f\"sub-{s}\",s,f\"{s}_{blocks[0]}\",\n",
    "        \"HilbAA_70to150_8band\",\"ecog_hilbAA70to150.fif\"), preload=True, verbose=False).info['ch_names']\n",
    "    if s == \"TCH14\":\n",
    "        ch_names = ch_names[:-1] # rm EKG channel\n",
    "    for i,c in enumerate(ch_names):\n",
    "        header.append([corrs[xm][s][i],xm,s,c]); header.append([corrs[ym][s][i],ym,s,c])\n",
    "csv_fname = os.path.join(git_path,\"stats\",\"lme\",\"csv\",f'{xm}_{ym}.csv') \n",
    "with open(csv_fname,'w+') as f:\n",
    "    csvWriter = csv.writer(f)\n",
    "    csvWriter.writerows(header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0e4533",
   "metadata": {},
   "source": [
    "## Convex non-negative matrix factorization (cNMF)\n",
    "cNMF is an approach that finds canonical response types based off \"soft clustering.\" Mathematically, it's a factorization method that decomposes a matrix of event-related activity into component functions that can be thought of as \"canonical responses.\" These components are dependent on both the magnitude of the response (HGA) and the latency of the responses. The equation is formalized as:\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "        $X\\approx\\hat{X}=FG^T$,\n",
    "    </center>\n",
    "</p>\n",
    "<p>\n",
    "    <center>\n",
    "        $F=XW$,\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "where $X$ is the neural time series of shape $n$ times x $p$ electrodes, and where $W$ is a matrix of shape $p$ electrodes x $k$ clusters and represents the cluster weights applied to the neural time series. $G$ is a matrix of shape $p$ electrodes x $k$ clusters and represents the weighting of individual electrodes within a cluster.\n",
    "\n",
    "This notebook makes such a matrix $X$ of high-gamma activity epoched to sentence onset, then calculates cNMF on it. In the context of this notebook, each cNMF component (or \"basis\") function represents a differential response to speaking and listening across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e115277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - Update locally!\n",
    "git_path = '/path/to/git/kurteff2024_code/'\n",
    "data_path = '/path/to/bids/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85e5da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import os\n",
    "import csv\n",
    "from tqdm.notebook import tqdm\n",
    "from img_pipe import img_pipe\n",
    "import sys\n",
    "import pymf3\n",
    "import h5py\n",
    "import warnings\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from matplotlib import rcParams as rc\n",
    "rc['pdf.fonttype'] = 42\n",
    "from matplotlib import cm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dca08b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_file = os.path.join(git_path, \"analysis\", \"cnmf\", \"h5\", \"NMF_grouped_nospkrall.hf5\")\n",
    "tmin, tmax = -1, 2\n",
    "level = 'sentence'\n",
    "baseline = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1a5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs = [s for s in os.listdir(\n",
    "    os.path.join(git_path,\"preprocessing\",\"events\",\"csv\")) if \"TCH\" in s or \"S0\" in s]\n",
    "exclude = [\"TCH8\"]\n",
    "no_imaging = [\"S0010\"]\n",
    "subjs = [s for s in subjs if s not in exclude]\n",
    "\n",
    "blocks = {\n",
    "    s: [\n",
    "        b.split(\"_\")[-1] for b in os.listdir(os.path.join(\n",
    "            git_path,\"analysis\",\"events\",\"csv\",s)) if f\"{s}_B\" in b and os.path.isfile(os.path.join(\n",
    "            git_path,\"analysis\",\"events\",\"csv\",s,b,f\"{b}_spkr_sn_all.txt\"\n",
    "        ))\n",
    "    ] for s in subjs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437857ea",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "045c3a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ch_names(subj,blocks,git_path,data_path):\n",
    "    blockid = \"_\".join([subj,blocks[s][0]])\n",
    "    if os.path.isfile(os.path.join(\n",
    "        data_path, \"ecog\", subj, blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "    )):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            ch_names = mne.io.read_raw_fif(os.path.join(\n",
    "                data_path, \"ecog\", subj, blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "            ),preload=False,verbose=False).info['ch_names']\n",
    "    else:\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                ch_names = mne.io.read_raw_fif(os.path.join(\n",
    "                    data_path, \"ecog\", f\"sub-{subj}\", blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "                ),preload=False,verbose=False).info['ch_names']\n",
    "        except:\n",
    "            raise Exception(\"Still cannot find raw. (Tried sub- prefix)\")\n",
    "    return ch_names\n",
    "def epoch_data(subj,blocks,git_path,data_path,\n",
    "               channel='spkr',level='sn',condition='all',\n",
    "               tmin=-.5, tmax=2, baseline=None,\n",
    "               set_picks=False,picks=None):\n",
    "    epochs = []\n",
    "    for b in blocks:\n",
    "        blockid = f'{subj}_{b}'\n",
    "        if os.path.isfile(os.path.join(\n",
    "            data_path, \"ecog\", subj, blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "        )):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                raw = mne.io.read_raw_fif(os.path.join(\n",
    "                    data_path, \"ecog\", subj, blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "                ),preload=True,verbose=False)\n",
    "        else:\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    raw = mne.io.read_raw_fif(os.path.join(\n",
    "                        data_path, \"ecog\", f\"sub-{subj}\", blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "                    ),preload=True,verbose=False)\n",
    "            except:\n",
    "                raise Exception(\"Still cannot find raw. (Tried sub- prefix)\")\n",
    "        fs = raw.info['sfreq']\n",
    "        if not set_picks:\n",
    "            picks = raw.info['ch_names']\n",
    "        onset_index, offset_index, id_index = 0,1,2\n",
    "        eventfile = os.path.join(\n",
    "            git_path,\"preprocessing\",\"events\",\"csv\",subj,blockid,\n",
    "            f\"{blockid}_{channel}_{level}_{condition}.txt\"\n",
    "        )\n",
    "        with open(eventfile,'r') as f:\n",
    "            r = csv.reader(f,delimiter='\\t')\n",
    "            events = np.array([[np.ceil(float(row[onset_index])*fs).astype(int),\n",
    "                                np.ceil(float(row[offset_index])*fs).astype(int),\n",
    "                                int(row[id_index])] for row in r]).astype(int)\n",
    "        if len(events) > 0:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                epochs.append(mne.Epochs(raw,events,picks=picks,tmin=tmin,tmax=tmax,baseline=baseline,preload=True,verbose=False))\n",
    "    if len(epochs) > 0:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return mne.concatenate_epochs(epochs)\n",
    "    else:\n",
    "        return None\n",
    "def epoch_other_events(subj,blocks,git_path,data_path,\n",
    "                      epoch_type=\"click\",tmin=-.5,tmax=2,baseline=None,set_picks=False,picks=None):\n",
    "    '''\n",
    "    Function for non-spkr/mic epochs.\n",
    "    Supported values for epoch_type: \"click\", \"text\"\n",
    "    '''\n",
    "    epochs = []\n",
    "    for b in blocks:\n",
    "        blockid = \"_\".join([subj,b])\n",
    "        if os.path.isfile(os.path.join(\n",
    "            data_path, \"ecog\", subj, blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "        )):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                raw = mne.io.read_raw_fif(os.path.join(\n",
    "                    data_path, \"ecog\", subj, blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "                ),preload=True,verbose=False)\n",
    "        else:\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    raw = mne.io.read_raw_fif(os.path.join(\n",
    "                        data_path, \"ecog\", f\"sub-{subj}\", blockid, \"HilbAA_70to150_8band\", \"ecog_hilbAA70to150.fif\"\n",
    "                    ),preload=True,verbose=False)\n",
    "            except:\n",
    "                raise Exception(\"Still cannot find raw. (Tried sub- prefix)\")\n",
    "        fs = raw.info['sfreq']\n",
    "        if not set_picks:\n",
    "            picks = raw.info['ch_names']\n",
    "        if epoch_type == \"click\":\n",
    "            eventfile = os.path.join(git_path,\"preprocessing\",\"events\",\"csv\",\n",
    "                                     subj,blockid,f\"{blockid}_click_eve.txt\")\n",
    "            if subj not in ['S0026','TCH14']:\n",
    "                onset_index, offset_index, id_index = 0,2,4\n",
    "            else:\n",
    "                onset_index, offset_index, id_index = 0,1,2\n",
    "        elif epoch_type == \"text\":\n",
    "            eventfile = os.path.join(git_path,\"preprocessing\",\"events\",\"csv\",subj,blockid,\n",
    "                                     f\"{blockid}_display_text.txt\")\n",
    "            onset_index, offset_index, id_index = 0,1,2\n",
    "        if os.path.isfile(eventfile):\n",
    "            with open(eventfile,'r') as f:\n",
    "                r = csv.reader(f,delimiter='\\t')\n",
    "                events = np.array([[np.ceil(float(row[onset_index])*fs).astype(int),\n",
    "                                    np.ceil(float(row[offset_index])*fs).astype(int),\n",
    "                                    float(row[id_index])] for row in r]).astype(int)\n",
    "        else:\n",
    "            warnings.warn(f\"No {epoch_type} events for {blockid}, skipping...\")\n",
    "            events = []\n",
    "        if len(events) > 0:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                epochs.append(mne.Epochs(raw,events,picks=picks,tmin=tmin,tmax=tmax,baseline=baseline,preload=True,verbose=False))\n",
    "    if len(epochs) > 0:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return mne.concatenate_epochs(epochs)\n",
    "    else:\n",
    "        return None\n",
    "def get_sig_chs(subj,git_path,ch_names,\n",
    "                p_thresh=0.05,nsubjs=16,nboots=1000,debug=False):\n",
    "    '''\n",
    "    Returns a subset of ch_names that only has p values below\n",
    "    the specified threshold.\n",
    "    '''\n",
    "    csv_fpath = os.path.join(git_path,\"stats\",\"bootstraps\",\n",
    "                             f\"seeg_elec_significance_{nsubjs}_subjs_{nboots}_boots.csv\")\n",
    "    pvals_df = pd.read_csv(csv_fpath)\n",
    "    sig_chs, dropped_chs = [], []\n",
    "    for ch in ch_names:\n",
    "        spkr_p = pvals_df.loc[\n",
    "            (pvals_df[\"subj\"]==subj)&(pvals_df[\"ch_name\"]==ch)\n",
    "        ][\"spkr_p\"].values[0]\n",
    "        mic_p = pvals_df.loc[\n",
    "            (pvals_df[\"subj\"]==subj)&(pvals_df[\"ch_name\"]==ch)\n",
    "        ][\"mic_p\"].values[0]\n",
    "        if spkr_p < p_thresh or mic_p < p_thresh:\n",
    "            sig_chs.append(ch)\n",
    "        else:\n",
    "            dropped_chs.append(ch)\n",
    "    if debug:\n",
    "        print(\n",
    "            f\"{subj}: Dropped {len(dropped_chs)} channels with p<{p_thresh}: {dropped_chs}\"\n",
    "        )\n",
    "    return sig_chs\n",
    "def get_elecs_outside_brain(subj,imaging_path,tch_imaging_path):\n",
    "    in_bolt_fpath = os.path.join(data_path,f\"{subj}_complete\",\"elecs\",f\"{subj}_IN_BOLT.txt\")\n",
    "    elecs_in_bolt = np.loadtxt(in_bolt_fpath, dtype=str, skiprows=1)\n",
    "    if len(elecs_in_bolt.shape) > 0:\n",
    "        if elecs_in_bolt.shape[0] != 0:\n",
    "            elecs_in_bolt = list(elecs_in_bolt)\n",
    "        else:\n",
    "            elecs_in_bolt = []\n",
    "    else:\n",
    "        elecs_in_bolt = [str(elecs_in_bolt)]\n",
    "    # Handle naming discrepancies to make ch names from imaging match the fif files\n",
    "    # Also drop bad channels\n",
    "    # This is kind of a hack, sorry lol\n",
    "    if subj == \"S0014\": # rename + bads\n",
    "        elecs_in_bolt = [e.replace(\"MAF-LOF\",\"MAFLOF\") for e in elecs_in_bolt if e not in [\"STG-HG10\",\"MTG-PH12\"]]\n",
    "    elif subj == \"S0015\": # bads\n",
    "        elecs_in_bolt = [e for e in elecs_in_bolt if e not in ['MMF-LOF16','SP-PI15','SP-PI16']]\n",
    "    elif subj == \"S0017\": # bads\n",
    "        elecs_in_bolt = [e for e in elecs_in_bolt if e not in ['ASF-MOF16', 'AMF-LOF14']]\n",
    "    return elecs_in_bolt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4ccbc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014091968536376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 16,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f092b2863766429faa7374f48294a73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subj S0004 has 81 channels, picking 38 of them...\n",
      "Loaded 93 epochs.\n",
      "Loaded 95 epochs.\n",
      "Loaded 93 epochs.\n",
      "Loaded 92 epochs.\n",
      "Loaded 186 epochs.\n",
      "Loaded 187 epochs.\n",
      "Subj S0006 has 104 channels, picking 29 of them...\n",
      "Loaded 114 epochs.\n",
      "Loaded 120 epochs.\n",
      "Loaded 114 epochs.\n",
      "Loaded 120 epochs.\n",
      "Loaded 228 epochs.\n",
      "Loaded 240 epochs.\n",
      "Subj S0007 has 96 channels, picking 39 of them...\n",
      "Loaded 93 epochs.\n",
      "Loaded 99 epochs.\n",
      "Loaded 94 epochs.\n",
      "Loaded 100 epochs.\n",
      "Loaded 188 epochs.\n",
      "Loaded 199 epochs.\n",
      "Subj S0010 has 146 channels, picking 6 of them...\n",
      "Loaded 81 epochs.\n",
      "Loaded 88 epochs.\n",
      "Loaded 84 epochs.\n",
      "Loaded 91 epochs.\n",
      "Loaded 165 epochs.\n",
      "Loaded 179 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfsh/anaconda3/envs/mne/lib/python3.7/site-packages/ipykernel_launcher.py:116: UserWarning: No text events for S0010_B1, skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subj S0014 has 103 channels, picking 37 of them...\n",
      "Loaded 37 epochs.\n",
      "Loaded 40 epochs.\n",
      "Loaded 38 epochs.\n",
      "Loaded 38 epochs.\n",
      "Loaded 75 epochs.\n",
      "Loaded 78 epochs.\n",
      "Subj S0015 has 121 channels, picking 60 of them...\n",
      "Loaded 28 epochs.\n",
      "Loaded 32 epochs.\n",
      "Loaded 35 epochs.\n",
      "Loaded 31 epochs.\n",
      "Loaded 63 epochs.\n",
      "Loaded 63 epochs.\n",
      "Subj S0017 has 125 channels, picking 79 of them...\n",
      "Loaded 47 epochs.\n",
      "Loaded 50 epochs.\n",
      "Loaded 45 epochs.\n",
      "Loaded 45 epochs.\n",
      "Loaded 92 epochs.\n",
      "Loaded 95 epochs.\n",
      "Subj S0018 has 142 channels, picking 46 of them...\n",
      "Loaded 92 epochs.\n",
      "Loaded 96 epochs.\n",
      "Loaded 93 epochs.\n",
      "Loaded 99 epochs.\n",
      "Loaded 185 epochs.\n",
      "Loaded 195 epochs.\n",
      "Subj S0019 has 100 channels, picking 68 of them...\n",
      "Loaded 96 epochs.\n",
      "Loaded 99 epochs.\n",
      "Loaded 93 epochs.\n",
      "Loaded 98 epochs.\n",
      "Loaded 189 epochs.\n",
      "Loaded 197 epochs.\n",
      "Subj S0020 has 106 channels, picking 53 of them...\n",
      "Loaded 95 epochs.\n",
      "Loaded 100 epochs.\n",
      "Loaded 94 epochs.\n",
      "Loaded 100 epochs.\n",
      "Loaded 189 epochs.\n",
      "Loaded 200 epochs.\n",
      "Subj S0021 has 108 channels, picking 13 of them...\n",
      "Loaded 44 epochs.\n",
      "Loaded 48 epochs.\n",
      "Loaded 45 epochs.\n",
      "Loaded 47 epochs.\n",
      "Loaded 89 epochs.\n",
      "Loaded 95 epochs.\n",
      "Subj S0023 has 142 channels, picking 53 of them...\n",
      "Loaded 24 epochs.\n",
      "Loaded 25 epochs.\n",
      "No spkr sh epochs for S0023\n",
      "No mic sh epochs for S0023\n",
      "Loaded 24 epochs.\n",
      "Loaded 25 epochs.\n",
      "Subj S0024 has 112 channels, picking 42 of them...\n",
      "Loaded 48 epochs.\n",
      "Loaded 49 epochs.\n",
      "Loaded 48 epochs.\n",
      "Loaded 50 epochs.\n",
      "Loaded 96 epochs.\n",
      "Loaded 99 epochs.\n",
      "Subj S0026 has 118 channels, picking 61 of them...\n",
      "Loaded 38 epochs.\n",
      "Loaded 40 epochs.\n",
      "Loaded 37 epochs.\n",
      "Loaded 38 epochs.\n",
      "Loaded 75 epochs.\n",
      "Loaded 78 epochs.\n",
      "Subj TCH06 has 186 channels, picking 76 of them...\n",
      "Loaded 50 epochs.\n",
      "Loaded 52 epochs.\n",
      "No spkr sh epochs for TCH06\n",
      "No mic sh epochs for TCH06\n",
      "Loaded 50 epochs.\n",
      "Loaded 52 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfsh/anaconda3/envs/mne/lib/python3.7/site-packages/ipykernel_launcher.py:116: UserWarning: No text events for TCH06_B21, skipping...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subj TCH14 has 255 channels, picking 107 of them...\n",
      "Loaded 46 epochs.\n",
      "Loaded 48 epochs.\n",
      "Loaded 44 epochs.\n",
      "Loaded 49 epochs.\n",
      "Loaded 90 epochs.\n",
      "Loaded 97 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kfsh/anaconda3/envs/mne/lib/python3.7/site-packages/ipykernel_launcher.py:116: UserWarning: No text events for TCH14_B15, skipping...\n"
     ]
    }
   ],
   "source": [
    "# Epoch the raw data from all participants\n",
    "excl_df = pd.read_csv(os.path.join(git_path,\"analysis\",\"all_excluded_electrodes.csv\"))\n",
    "conditions = ['el','sh','all']\n",
    "channels = ['spkr', 'mic']\n",
    "epochs = dict()\n",
    "for s in tqdm(subjs):\n",
    "    chs = get_ch_names(s,blocks,git_path,data_path)\n",
    "    excl_ch_names = list(excl_df.loc[excl_df['subject']==s]['channel'].values)\n",
    "    picks = [ch for ch in chs if ch not in excl_ch_names]\n",
    "    print(f\"Subj {s} has {len(chs)} channels, picking {len(picks)} of them...\")\n",
    "    if len(picks) > 0:\n",
    "        epochs[s] = dict()\n",
    "        for cond in conditions:\n",
    "            epochs[s][cond] = dict()\n",
    "            for channel in channels:\n",
    "                epochs[s][cond][channel] = epoch_data(\n",
    "                    s, blocks[s], git_path, data_path, channel=channel, level='sn', condition=cond,\n",
    "                    tmin=tmin, tmax=tmax, baseline=baseline, picks=picks, set_picks=True\n",
    "                )\n",
    "                if epochs[s][cond][channel] is not None:\n",
    "                    print(f\"Loaded {len(epochs[s][cond][channel])} epochs.\")\n",
    "                else:\n",
    "                    print(f\"No {channel} {cond} epochs for {s}\")\n",
    "        # Epoch clicks, display_text, display_cross\n",
    "        epochs[s]['click'] = epoch_other_events(\n",
    "            s, blocks[s], git_path, data_path, epoch_type=\"click\",\n",
    "            tmin=tmin, tmax=tmax, baseline=baseline, picks=picks, set_picks=True\n",
    "        )\n",
    "        epochs[s]['text'] = epoch_other_events(\n",
    "            s, blocks[s], git_path, data_path, epoch_type=\"text\",\n",
    "            tmin=tmin, tmax=tmax, baseline=baseline, picks=picks, set_picks=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934f4a0",
   "metadata": {},
   "source": [
    "## Format $X$\n",
    "\n",
    "We are making a large $n$ samples x $p$ electrodes matrix by concatenating across all subjects. $n$ is event-related to sentence onset and is just all the samples in our tmin-tmax range. Channels are kept separate ($p$) while we average across epochs within $p$. \n",
    "\n",
    "<center>\n",
    "    $X_p = \\frac{1}{n}\\sum\\limits^{n=2}_{n=-1}H\\gamma_{p,n}$\n",
    "</center>\n",
    "<p></p>\n",
    "<center>\n",
    "    $X_s = [X_{p1}|X_{p2}|...|X_{pn}]$\n",
    "</center>\n",
    "<p></p>\n",
    "<center>\n",
    "    $X = [X_{s1}|X_{s2}|...|X_{sn}]$\n",
    "</center>\n",
    "\n",
    "We will make one $X$ per condition (`spkr`, `mic`, `echo`, `shuff`) and store as a `dict`. We'll save this to a Pandas DataFrame as well for easy access.\n",
    "\n",
    "We also are going to save anatomical information to this `df`. So let's grab that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e67ee756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01587200164794922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 16,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92ab15e5d9e45ba91a30a4e1138b5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0019: Dropping these channels as they aren't present in the anat: LAT2, LAT3, LAT4, LAT5, LPF1, LPF2, LPF3, LPF5, LPF6, LPF7, LPF8\n"
     ]
    }
   ],
   "source": [
    "imaging = dict()\n",
    "for s in tqdm(subjs):\n",
    "    if s not in no_imaging:\n",
    "        imaging[s] = dict()\n",
    "        patient = img_pipe.freeCoG(f\"{s}_complete\", hem=\"stereo\")\n",
    "        an = patient.get_elecs(elecfile_prefix=\"TDT_elecs_all_warped\")['anatomy']\n",
    "        fs_ch_names = [a[0][0].replace(\"-\",\"\") for a in an]\n",
    "        if s == \"S0020\":\n",
    "            # One device for S0020 is named incorrectly so we have to write an exception for it.\n",
    "            fs_ch_names = [c.replace(\"APIOF'\",\"AIPOF'\") for c in fs_ch_names]\n",
    "        fif_ch_names = epochs[s]['all']['spkr'].info['ch_names']\n",
    "        fs_idxs, fif_idxs, dropped_chs = [], [], []\n",
    "        for fif_i, ch_name in enumerate(fif_ch_names):\n",
    "            if ch_name.replace(\"-\",\"\") in fs_ch_names:\n",
    "                fif_idxs.append(fif_i)\n",
    "                fs_i = fs_ch_names.index(ch_name.replace(\"-\",\"\"))\n",
    "                fs_idxs.append(fs_i)\n",
    "            else:\n",
    "                dropped_chs.append(ch_name)\n",
    "        if len(dropped_chs) > 0:\n",
    "            print(\n",
    "                f\"{s}: Dropping these channels as they aren't present in the anat: {', '.join(dropped_chs)}\"\n",
    "            )\n",
    "        imaging[s]['ch_names'] = list(np.array([a[0][0] for a in an])[fs_idxs])\n",
    "        imaging[s]['elec_surf'] = patient.get_elecs(elecfile_prefix=\"TDT_elecs_all_warped\")['elecmatrix'][fs_idxs]\n",
    "        imaging[s]['elec_mri'] = cnmf_utils.tkRAS_to_MNI(imaging[s]['elec_surf'])\n",
    "        imaging[s]['anat'] = list(np.array([cnmf_utisl.get_anatomy_short(a[3][0]) for a in an])[fs_idxs])\n",
    "        imaging[s]['fs_idxs'] = fs_idxs\n",
    "        imaging[s]['fif_idxs'] = fif_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39567f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spkr': [], 'mic': [], 'el': [], 'sh': ['S0023', 'TCH06'], 'click': [], 'text': ['S0010']}\n"
     ]
    }
   ],
   "source": [
    "nmf_X = dict()\n",
    "missing_types = {\n",
    "    epoch_type:[] for epoch_type in ['spkr','mic','el','sh','click','text']\n",
    "}\n",
    "for s in subjs:\n",
    "    nmf_X[s] = dict()\n",
    "    for epoch_type in ['spkr', 'mic', 'el', 'sh', 'click', 'text']:\n",
    "        nmf_X[s][epoch_type] = dict()\n",
    "        if epoch_type in ['spkr', 'mic']:\n",
    "            if epochs[s]['all'][epoch_type] is not None:\n",
    "                if s not in no_imaging:\n",
    "                    nmf_X[s][epoch_type]['resp'] = epochs[s]['all'][epoch_type].get_data()[:,imaging[s]['fif_idxs'],:].mean(0)\n",
    "                    nmf_X[s][epoch_type]['sem'] = cnmf_utils.sem(epochs[s]['all'][epoch_type].get_data()[:,imaging[s]['fif_idxs'],:])\n",
    "                else:\n",
    "                    # Just use all channels\n",
    "                    nmf_X[s][epoch_type]['resp'] = epochs[s]['all'][epoch_type].get_data().mean(0)\n",
    "                    nmf_X[s][epoch_type]['sem'] = cnmf_utils.sem(epochs[s]['all'][epoch_type].get_data())\n",
    "            else:\n",
    "                nmf_X[s][epoch_type]['resp'] = None\n",
    "                nmf_X[s][epoch_type]['sem'] = None\n",
    "                missing_types[epoch_type].append(s)\n",
    "        elif epoch_type in ['el', 'sh']:\n",
    "            if epochs[s][epoch_type]['spkr'] is not None:\n",
    "                if s not in no_imaging:\n",
    "                    nmf_X[s][epoch_type]['resp'] = epochs[s][epoch_type]['spkr'].get_data()[:,imaging[s]['fif_idxs'],:].mean(0)\n",
    "                    nmf_X[s][epoch_type]['sem'] = cnmf_utils.sem(epochs[s][epoch_type]['spkr'].get_data()[:,imaging[s]['fif_idxs'],:])\n",
    "                else:\n",
    "                    # Just use all channels\n",
    "                    nmf_X[s][epoch_type]['resp'] = epochs[s][epoch_type]['spkr'].get_data().mean(0)\n",
    "                    nmf_X[s][epoch_type]['sem'] = cnmf_utils.sem(epochs[s][epoch_type]['spkr'].get_data())\n",
    "            else:\n",
    "                nmf_X[s][epoch_type]['resp'] = None\n",
    "                nmf_X[s][epoch_type]['sem'] = None\n",
    "                missing_types[epoch_type].append(s)\n",
    "        else:\n",
    "            if epochs[s][epoch_type] is not None:\n",
    "                if s not in no_imaging:\n",
    "                    nmf_X[s][epoch_type]['resp'] = epochs[s][epoch_type].get_data()[:,imaging[s]['fif_idxs'],:].mean(0)\n",
    "                    nmf_X[s][epoch_type]['sem'] = cnmf_utils.sem(epochs[s][epoch_type].get_data()[:,imaging[s]['fif_idxs'],:])\n",
    "                else:\n",
    "                    # Just use all channels\n",
    "                    nmf_X[s][epoch_type]['resp'] = epochs[s][epoch_type].get_data().mean(0)\n",
    "                    nmf_X[s][epoch_type]['sem'] = cnmf_utils.sem(epochs[s][epoch_type].get_data())\n",
    "            else:\n",
    "                nmf_X[s][epoch_type]['resp'] = None\n",
    "                nmf_X[s][epoch_type]['sem'] = None\n",
    "                missing_types[epoch_type].append(s)\n",
    "print(missing_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "123ef604",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bases = np.arange(2,29)\n",
    "nmf, percent_variance = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b9b9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bname = 'spkrmicclick'\n",
    "nmf_fpath = os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}.hf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22dea00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(796, 903)\n"
     ]
    }
   ],
   "source": [
    "all_subjs[bname],resp[bname],stderr[bname],elecs[bname],anat[bname],ch_names[bname] = dict(),dict(),dict(\n",
    "    ),dict(),dict(),dict()\n",
    "for epoch_type in ['spkr','mic','click']:\n",
    "    als, r, se, el, an, cn = [], [], [], [], [], []\n",
    "    for s in subjs:\n",
    "        if s not in no_imaging:\n",
    "            for i in np.arange(len(imaging[s]['fif_idxs'])):\n",
    "                als.append(s)\n",
    "                r.append(nmf_X[s][epoch_type]['resp'][i,:])\n",
    "                se.append((nmf_X[s][epoch_type]['sem'][0][i,:],nmf_X[s][epoch_type]['sem'][1][i,:]))\n",
    "                el.append(imaging[s]['elec_surf'][i])\n",
    "                an.append(imaging[s]['anat'][i])\n",
    "                cn.append(imaging[s]['ch_names'][i])\n",
    "        else:\n",
    "            for i in np.arange(nmf_X[s][epoch_type]['resp'].shape[0]):\n",
    "                als.append(s)\n",
    "                r.append(nmf_X[s][epoch_type]['resp'][i,:])\n",
    "                se.append((nmf_X[s][epoch_type]['sem'][0][i,:],nmf_X[s][epoch_type]['sem'][1][i,:]))\n",
    "                el.append(np.zeros(3))\n",
    "                an.append('Anatomy unavailable')\n",
    "                cn.append(epochs[s]['all']['spkr'].info['ch_names'][i])\n",
    "    all_subjs[bname][epoch_type] = np.hstack((als))\n",
    "    resp[bname][epoch_type] = np.vstack((r))\n",
    "    stderr[bname][epoch_type] = np.array(se)\n",
    "    elecs[bname][epoch_type] = np.vstack((el))\n",
    "    anat[bname][epoch_type] = np.hstack((an))\n",
    "    ch_names[bname][epoch_type] = np.hstack((cn))\n",
    "X = np.hstack((resp[bname]['mic'],resp[bname]['spkr'],resp[bname]['click']))\n",
    "print(X.shape) # N channels x N times*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "daeb65ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 clusters, PVE=63.32 percent variance explained\n",
      "3 clusters, PVE=76.98 percent variance explained\n",
      "4 clusters, PVE=78.42 percent variance explained\n",
      "5 clusters, PVE=79.95 percent variance explained\n",
      "6 clusters, PVE=81.24 percent variance explained\n",
      "7 clusters, PVE=83.28 percent variance explained\n",
      "8 clusters, PVE=84.82 percent variance explained\n",
      "9 clusters, PVE=85.61 percent variance explained\n",
      "10 clusters, PVE=86.12 percent variance explained\n",
      "11 clusters, PVE=86.64 percent variance explained\n",
      "12 clusters, PVE=87.04 percent variance explained\n",
      "13 clusters, PVE=87.43 percent variance explained\n",
      "14 clusters, PVE=87.46 percent variance explained\n",
      "15 clusters, PVE=87.87 percent variance explained\n",
      "16 clusters, PVE=88.38 percent variance explained\n",
      "17 clusters, PVE=88.30 percent variance explained\n",
      "18 clusters, PVE=88.37 percent variance explained\n",
      "19 clusters, PVE=89.19 percent variance explained\n",
      "20 clusters, PVE=89.30 percent variance explained\n",
      "21 clusters, PVE=89.53 percent variance explained\n",
      "22 clusters, PVE=89.59 percent variance explained\n",
      "23 clusters, PVE=89.54 percent variance explained\n",
      "24 clusters, PVE=89.96 percent variance explained\n",
      "25 clusters, PVE=89.97 percent variance explained\n",
      "26 clusters, PVE=90.14 percent variance explained\n",
      "27 clusters, PVE=90.39 percent variance explained\n",
      "28 clusters, PVE=90.73 percent variance explained\n"
     ]
    }
   ],
   "source": [
    "nmf[bname] = dict()\n",
    "percent_variance[bname] = []\n",
    "if not os.path.isfile(nmf_fpath):\n",
    "    for nb in num_bases:\n",
    "        nmf[bname][nb] = pymf3.convexNMF(X,W=None,H=None,num_bases=nb)\n",
    "        nmf[bname][nb].factorize()\n",
    "        percent_variance[bname].append(cnmf_utils.pve(X, nmf[bname][nb]))\n",
    "        print(f\"{nb} clusters, PVE={percent_variance[bname][-1]*100:.2f} percent variance explained\")\n",
    "    with h5py.File(nmf_fpath, 'w') as f:\n",
    "        f.create_dataset(\"/times\", data=epochs['S0004']['all']['spkr'].times)\n",
    "        f.create_dataset(\"/resp/spkr\", data=resp[bname]['spkr'])\n",
    "        f.create_dataset(\"/resp/mic\", data=resp[bname]['mic'])\n",
    "        f.create_dataset(\"/resp/click\", data=resp[bname]['click'])\n",
    "        f.create_dataset(\"/pve\", data=percent_variance[bname])\n",
    "        f.create_dataset(\"/num_bases\", data=num_bases)\n",
    "        f.create_dataset(\"/elecs\", data=elecs[bname]['spkr'])\n",
    "        for nb in num_bases:\n",
    "            f.create_dataset(f\"/{nb}_bases/W\", data=nmf[bname][nb].W)\n",
    "            f.create_dataset(f\"/{nb}_bases/H\", data=nmf[bname][nb].H)\n",
    "        # h5py can't save strings for some fucked up reason so we will save it as a separate textfile\n",
    "        np.savetxt(os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}_all_subjs.txt\"\n",
    "                               ), all_subjs[bname]['spkr'], fmt=\"%s\")\n",
    "        np.savetxt(os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}_anat.txt\"\n",
    "                               ), anat[bname]['spkr'], fmt=\"%s\")\n",
    "        np.savetxt(os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}_ch_names.txt\"\n",
    "                               ), ch_names[bname]['spkr'], fmt=\"%s\")\n",
    "else:\n",
    "    print(\"Loading NMF from file\")\n",
    "    for nb in num_bases:\n",
    "        nmf[bname][nb] = pymf3.convexNMF(X,W=None,H=None,num_bases=nb)\n",
    "        with h5py.File(nmf_fpath,'r') as f:\n",
    "            nmf[bname][nb].W = f[f\"/{nb}_bases/W\"][:]\n",
    "            nmf[bname][nb].H = f[f\"/{nb}_bases/H\"][:]\n",
    "            percent_variance[bname].append(f[\"/pve\"][nb-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba7630a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bname = 'micelshuffclick'\n",
    "nmf_fpath = os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}.hf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7affbf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667, 1204)\n"
     ]
    }
   ],
   "source": [
    "all_subjs[bname],resp[bname],stderr[bname],elecs[bname],anat[bname],ch_names[bname] = dict(),dict(),dict(\n",
    "    ),dict(),dict(),dict()\n",
    "for epoch_type in ['mic','el','sh','click']:\n",
    "    als, r, se, el, an, cn = [], [], [], [], [], []\n",
    "    for s in subjs:\n",
    "        if s not in np.hstack((missing_types['mic'], missing_types['el'], missing_types['sh'])):\n",
    "            if s not in no_imaging:\n",
    "                for i in np.arange(len(imaging[s]['fif_idxs'])):\n",
    "                    als.append(s)\n",
    "                    r.append(nmf_X[s][epoch_type]['resp'][i,:])\n",
    "                    se.append((nmf_X[s][epoch_type]['sem'][0][i,:],nmf_X[s][epoch_type]['sem'][1][i,:]))\n",
    "                    el.append(imaging[s]['elec_surf'][i])\n",
    "                    an.append(imaging[s]['anat'][i])\n",
    "                    cn.append(imaging[s]['ch_names'][i])\n",
    "            else:\n",
    "                for i in np.arange(nmf_X[s][epoch_type]['resp'].shape[0]):\n",
    "                    als.append(s)\n",
    "                    r.append(nmf_X[s][epoch_type]['resp'][i,:])\n",
    "                    se.append((nmf_X[s][epoch_type]['sem'][0][i,:],nmf_X[s][epoch_type]['sem'][1][i,:]))\n",
    "                    el.append(np.zeros(3))\n",
    "                    an.append('Anatomy unavailable')\n",
    "                    cn.append(epochs[s]['all']['spkr'].info['ch_names'][i])\n",
    "    all_subjs[bname][epoch_type] = np.hstack((als))\n",
    "    resp[bname][epoch_type] = np.vstack((r))\n",
    "    stderr[bname][epoch_type] = np.array(se)\n",
    "    elecs[bname][epoch_type] = np.vstack((el))\n",
    "    anat[bname][epoch_type] = np.hstack((an))\n",
    "    ch_names[bname][epoch_type] = np.hstack((cn))\n",
    "X = np.hstack((resp[bname]['mic'],resp[bname]['el'],resp[bname]['sh'],resp[bname]['click']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c162aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 clusters, PVE=62.50 percent variance explained\n",
      "3 clusters, PVE=73.17 percent variance explained\n",
      "4 clusters, PVE=77.68 percent variance explained\n",
      "5 clusters, PVE=79.34 percent variance explained\n",
      "6 clusters, PVE=81.40 percent variance explained\n",
      "7 clusters, PVE=82.54 percent variance explained\n",
      "8 clusters, PVE=83.48 percent variance explained\n",
      "9 clusters, PVE=84.42 percent variance explained\n",
      "10 clusters, PVE=85.40 percent variance explained\n",
      "11 clusters, PVE=85.88 percent variance explained\n",
      "12 clusters, PVE=86.10 percent variance explained\n",
      "13 clusters, PVE=86.34 percent variance explained\n",
      "14 clusters, PVE=87.14 percent variance explained\n",
      "15 clusters, PVE=87.18 percent variance explained\n",
      "16 clusters, PVE=87.15 percent variance explained\n",
      "17 clusters, PVE=87.47 percent variance explained\n",
      "18 clusters, PVE=87.90 percent variance explained\n",
      "19 clusters, PVE=88.16 percent variance explained\n",
      "20 clusters, PVE=87.86 percent variance explained\n",
      "21 clusters, PVE=88.24 percent variance explained\n",
      "22 clusters, PVE=88.45 percent variance explained\n",
      "23 clusters, PVE=88.83 percent variance explained\n",
      "24 clusters, PVE=89.03 percent variance explained\n",
      "25 clusters, PVE=89.08 percent variance explained\n",
      "26 clusters, PVE=89.56 percent variance explained\n",
      "27 clusters, PVE=89.43 percent variance explained\n",
      "28 clusters, PVE=89.73 percent variance explained\n"
     ]
    }
   ],
   "source": [
    "nmf[bname] = dict()\n",
    "percent_variance[bname] = []\n",
    "if not os.path.isfile(nmf_fpath):\n",
    "    for nb in num_bases:\n",
    "        nmf[bname][nb] = pymf3.convexNMF(X,W=None,H=None,num_bases=nb)\n",
    "        nmf[bname][nb].factorize()\n",
    "        percent_variance[bname].append(cnmf_utils.pve(X, nmf[bname][nb]))\n",
    "        print(f\"{nb} clusters, PVE={percent_variance[bname][-1]*100:.2f} percent variance explained\")\n",
    "    with h5py.File(nmf_fpath, 'w') as f:\n",
    "        f.create_dataset(\"/times\", data=epochs['S0004']['all']['spkr'].times)\n",
    "        f.create_dataset(\"/resp/mic\", data=resp[bname]['mic'])\n",
    "        f.create_dataset(\"/resp/el\", data=resp[bname]['el'])\n",
    "        f.create_dataset(\"/resp/sh\", data=resp[bname]['sh'])\n",
    "        f.create_dataset(\"/resp/click\", data=resp[bname]['click'])\n",
    "        f.create_dataset(\"/pve\", data=percent_variance[bname])\n",
    "        f.create_dataset(\"/num_bases\", data=num_bases)\n",
    "        f.create_dataset(\"/elecs\", data=elecs[bname]['mic'])\n",
    "        for nb in num_bases:\n",
    "            f.create_dataset(f\"/{nb}_bases/W\", data=nmf[bname][nb].W)\n",
    "            f.create_dataset(f\"/{nb}_bases/H\", data=nmf[bname][nb].H)\n",
    "        # h5py can't save strings for some fucked up reason so we will save it as a separate textfile\n",
    "        np.savetxt(os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}_all_subjs.txt\"\n",
    "                               ), all_subjs[bname]['spkr'], fmt=\"%s\")\n",
    "        np.savetxt(os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}_anat.txt\"\n",
    "                               ), anat[bname]['spkr'], fmt=\"%s\")\n",
    "        np.savetxt(os.path.join(git_path,\"analysis\",\"cnmf\",\"h5\",f\"NMF_grouped_{bname}_ch_names.txt\"\n",
    "                               ), ch_names[bname]['spkr'], fmt=\"%s\")\n",
    "else:\n",
    "    print(\"Loading NMF from file\")\n",
    "    for nb in num_bases:\n",
    "        nmf[bname][nb] = pymf3.convexNMF(X,W=None,H=None,num_bases=nb)\n",
    "        with h5py.File(nmf_fpath,'r') as f:\n",
    "            nmf[bname][nb].W = f[f\"/{nb}_bases/W\"][:]\n",
    "            nmf[bname][nb].H = f[f\"/{nb}_bases/H\"][:]\n",
    "            percent_variance[bname].append(f[\"/pve\"][nb-2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
